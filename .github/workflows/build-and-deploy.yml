name: Build and Deploy Spark Pipeline

on:
  # Se d√©clenche apr√®s le succ√®s de Terraform Apply
  workflow_run:
    workflows: ["Terraform Apply"]
    types:
      - completed
    branches:
      - main
  # Se d√©clenche aussi sur push vers main
  # Note: On ne filtre plus les changements Terraform pour permettre le build apr√®s Apply
  push:
    branches:
      - main
  # Se d√©clenche sur merge de PR vers main
  pull_request:
    types: [closed]
    branches:
      - main
  workflow_dispatch:  # Permet de d√©clencher manuellement
  # D√©clenchement alternatif : apr√®s un push vers main (m√™me avec Terraform)
  # Cela garantit que le build s'ex√©cute m√™me si workflow_run ne fonctionne pas

env:
  PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
  REGION: europe-west1

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    # Ne s'ex√©cute que si :
    # - C'est un workflow_dispatch (d√©clenchement manuel) - TOUJOURS autoris√©
    # - C'est un push vers main - TOUJOURS autoris√© (m√™me avec changements Terraform)
    # - C'est un merge de PR vers main - TOUJOURS autoris√©
    # - C'est d√©clench√© par workflow_run ET le workflow Terraform Apply a r√©ussi
    # 
    # Note: On autorise le push vers main m√™me avec Terraform pour garantir que le build
    # s'ex√©cute apr√®s un Terraform Apply r√©ussi, m√™me si workflow_run ne se d√©clenche pas
    if: |
      github.event_name == 'workflow_dispatch' ||
      (github.event_name == 'push' && (github.ref == 'refs/heads/main' || github.ref == 'refs/heads/master')) ||
      (github.event_name == 'pull_request' && github.event.pull_request.merged == true) ||
      (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success')
    
    steps:
      - name: Debug workflow trigger
        run: |
          echo "üîç Informations de debug du workflow Build JAR:"
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          echo "Event name: ${{ github.event_name }}"
          echo "Ref: ${{ github.ref }}"
          echo "Branch: ${{ github.ref_name }}"
          echo ""
          
          if [ "${{ github.event_name }}" == "workflow_run" ]; then
            echo "üìã Informations du workflow_run:"
            echo "  Workflow name: ${{ github.event.workflow_run.name }}"
            echo "  Workflow conclusion: ${{ github.event.workflow_run.conclusion }}"
            echo "  Workflow status: ${{ github.event.workflow_run.status }}"
            echo "  Head branch: ${{ github.event.workflow_run.head_branch }}"
            echo "  Head SHA: ${{ github.event.workflow_run.head_sha }}"
            echo ""
            if [ "${{ github.event.workflow_run.conclusion }}" == "success" ]; then
              echo "‚úÖ Terraform Apply a r√©ussi - Le build JAR va s'ex√©cuter"
            else
              echo "‚ö†Ô∏è  Terraform Apply a √©chou√© ou n'est pas termin√©"
              echo "   Conclusion: ${{ github.event.workflow_run.conclusion }}"
              echo "   Status: ${{ github.event.workflow_run.status }}"
            fi
          fi
          
          if [ "${{ github.event_name }}" == "pull_request" ]; then
            echo "üìã Informations de la Pull Request:"
            echo "  Merged: ${{ github.event.pull_request.merged }}"
            echo "  Base branch: ${{ github.event.pull_request.base.ref }}"
          fi
          
          if [ "${{ github.event_name }}" == "push" ]; then
            echo "üìã Informations du push:"
            echo "  Branch: ${{ github.ref_name }}"
            echo "  SHA: ${{ github.sha }}"
          fi
          
          if [ "${{ github.event_name }}" == "workflow_dispatch" ]; then
            echo "üìã D√©clenchement manuel"
          fi
          
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          # Si d√©clench√© par workflow_run, utiliser le commit du workflow qui a d√©clench√©
          ref: ${{ github.event.workflow_run.head_branch || github.ref }}
          sha: ${{ github.event.workflow_run.head_sha || github.sha }}
      
      - name: Set up JDK 11
        uses: actions/setup-java@v4
        with:
          java-version: '11'
          distribution: 'temurin'
          cache: 'sbt'
      
      - name: Setup Scala
        uses: olafurpg/setup-scala@v13
        with:
          java-version: '11'
      
      - name: Cache SBT dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.sbt
            ~/.ivy2/cache
            project/target
          key: ${{ runner.os }}-sbt-${{ hashFiles('**/*.sbt') }}-${{ hashFiles('project/build.properties') }}
          restore-keys: |
            ${{ runner.os }}-sbt-
      
      - name: Build JAR with SBT
        run: |
          sbt clean assembly
      
      - name: Build dependencies JAR
        run: |
          sbt "project deps" assembly
      
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
      
      - name: Upload JAR to GCS
        run: |
          # Construction du nom du bucket (remplacement des points par des tirets dans le project_id)
          PROJECT_ID_FOR_BUCKET=$(echo "${{ secrets.GCP_PROJECT_ID }}" | tr '.' '-')
          GCS_BUCKET="spark-streaming-pipeline-dev-${PROJECT_ID_FOR_BUCKET}-artifacts"
          
          VERSION=$(git rev-parse --short HEAD)
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          JAR_NAME="spark-streaming-local-assembly-${VERSION}-${TIMESTAMP}.jar"
          
          echo "üì¶ Upload du JAR vers: gs://${GCS_BUCKET}/jars/${JAR_NAME}"
          
          # Upload du JAR principal
          gsutil cp target/scala-2.12/spark-streaming-local-assembly-*.jar \
            gs://${GCS_BUCKET}/jars/${JAR_NAME}
          
          # Cr√©er un lien symbolique vers latest
          gsutil cp gs://${GCS_BUCKET}/jars/${JAR_NAME} \
            gs://${GCS_BUCKET}/jars/spark-streaming-local-assembly-latest.jar
          
          echo "‚úÖ JAR upload√©: gs://${GCS_BUCKET}/jars/${JAR_NAME}"
          echo "JAR_LATEST=gs://${GCS_BUCKET}/jars/spark-streaming-local-assembly-latest.jar" >> $GITHUB_ENV
      
      - name: Cleanup old JAR versions
        run: |
          # Construction du nom du bucket
          PROJECT_ID_FOR_BUCKET=$(echo "${{ secrets.GCP_PROJECT_ID }}" | tr '.' '-')
          GCS_BUCKET="spark-streaming-pipeline-dev-${PROJECT_ID_FOR_BUCKET}-artifacts"
          
          echo "üßπ Nettoyage des anciennes versions de JARs..."
          echo "  üìã Strat√©gie: Garder uniquement latest.jar, supprimer toutes les autres versions"
          
          # Lister tous les JARs versionn√©s (exclure latest.jar)
          JARS=$(gsutil ls gs://${GCS_BUCKET}/jars/spark-streaming-local-assembly-*.jar 2>/dev/null | \
            grep -v "latest.jar" | \
            sort -r)
          
          # Compter le nombre de JARs √† supprimer
          JAR_COUNT=$(echo "$JARS" | grep -c "\.jar$" || echo "0")
          
          if [ "$JAR_COUNT" -gt 0 ]; then
            echo "  üìä Total: $JAR_COUNT ancienne(s) version(s) trouv√©e(s)"
            echo "  üóëÔ∏è  Suppression de toutes les anciennes versions..."
            
            # Supprimer tous les JARs versionn√©s (garder seulement latest.jar)
            echo "$JARS" | while read -r jar; do
              if [ -n "$jar" ]; then
                echo "    üóëÔ∏è  Suppression: $(basename $jar)"
                gsutil rm "$jar" || echo "    ‚ö†Ô∏è  √âchec de la suppression: $jar"
              fi
            done
            echo "  ‚úÖ Nettoyage termin√© - Seul latest.jar est conserv√©"
          else
            echo "  ‚ÑπÔ∏è  Aucune ancienne version trouv√©e, seul latest.jar existe"
          fi
          
          # V√©rifier que latest.jar existe
          if gsutil ls gs://${GCS_BUCKET}/jars/spark-streaming-local-assembly-latest.jar >/dev/null 2>&1; then
            echo "  ‚úÖ latest.jar confirm√©: gs://${GCS_BUCKET}/jars/spark-streaming-local-assembly-latest.jar"
          else
            echo "  ‚ö†Ô∏è  Attention: latest.jar n'existe pas encore"
          fi
      
      - name: Upload dependencies JAR
        run: |
          # Construction du nom du bucket (remplacement des points par des tirets dans le project_id)
          PROJECT_ID_FOR_BUCKET=$(echo "${{ secrets.GCP_PROJECT_ID }}" | tr '.' '-')
          GCS_BUCKET="spark-streaming-pipeline-dev-${PROJECT_ID_FOR_BUCKET}-artifacts"
          
          if [ -f deps/target/scala-2.12/spark-streaming-deps.jar ]; then
            echo "üì¶ Upload des d√©pendances vers: gs://${GCS_BUCKET}/jars/deps/"
            gsutil cp deps/target/scala-2.12/spark-streaming-deps.jar \
              gs://${GCS_BUCKET}/jars/deps/spark-streaming-deps.jar
            echo "‚úÖ D√©pendances upload√©es"
          else
            echo "‚ö†Ô∏è  Fichier de d√©pendances non trouv√©, ignor√©"
          fi
      
      - name: Display deployment info
        run: |
          echo "‚úÖ Build termin√© avec succ√®s"
          echo "üì¶ JAR disponible: ${{ env.JAR_LATEST }}"
          echo "üîß Pour lancer le job Dataproc, utilisez:"
          echo "   gcloud dataproc jobs submit spark \\"
          echo "     --region=${REGION} \\"
          echo "     --class=com.example.streaming.Consumer \\"
          echo "     --jars=${{ env.JAR_LATEST }} \\"
          echo "     -- <PROJECT_ID> <INPUT_PATH> <BIGQUERY_TABLE> <CHECKPOINT_PATH>"



