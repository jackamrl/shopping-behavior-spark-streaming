name: Build and Deploy Spark Pipeline

on:
  # Se dÃ©clenche aprÃ¨s le succÃ¨s de Terraform Apply
  workflow_run:
    workflows: ["Terraform Apply"]
    types:
      - completed
    branches:
      - main
  # Se dÃ©clenche aussi sur push vers main (sauf changements Terraform)
  push:
    branches:
      - main
    paths-ignore:
      - 'terraform/**'
      - '.github/workflows/terraform-*.yml'
  # Se dÃ©clenche sur merge de PR vers main (sauf changements Terraform)
  pull_request:
    types: [closed]
    branches:
      - main
    paths-ignore:
      - 'terraform/**'
      - '.github/workflows/terraform-*.yml'
  workflow_dispatch:  # Permet de dÃ©clencher manuellement

env:
  PROJECT_ID: ${{ secrets.GCP_PROJECT_ID }}
  GCS_BUCKET: ${{ secrets.GCS_ARTIFACTS_BUCKET }}
  REGION: europe-west1

jobs:
  build-and-deploy:
    runs-on: ubuntu-latest
    # Ne s'exÃ©cute que si :
    # - C'est un push vers main (sauf Terraform)
    # - C'est un merge de PR vers main (sauf Terraform)
    # - C'est dÃ©clenchÃ© par workflow_run ET le workflow Terraform Apply a rÃ©ussi
    if: |
      (github.event_name == 'push') ||
      (github.event_name == 'pull_request' && github.event.pull_request.merged == true) ||
      (github.event_name == 'workflow_run' && github.event.workflow_run.conclusion == 'success')
    
    steps:
      - name: Checkout code
        uses: actions/checkout@v4
        with:
          # Si dÃ©clenchÃ© par workflow_run, utiliser le commit du workflow qui a dÃ©clenchÃ©
          ref: ${{ github.event.workflow_run.head_branch || github.ref }}
          sha: ${{ github.event.workflow_run.head_sha || github.sha }}
      
      - name: Set up JDK 11
        uses: actions/setup-java@v4
        with:
          java-version: '11'
          distribution: 'temurin'
          cache: 'sbt'
      
      - name: Setup Scala
        uses: olafurpg/setup-scala@v13
        with:
          java-version: '11'
      
      - name: Cache SBT dependencies
        uses: actions/cache@v3
        with:
          path: |
            ~/.sbt
            ~/.ivy2/cache
            project/target
          key: ${{ runner.os }}-sbt-${{ hashFiles('**/*.sbt') }}-${{ hashFiles('project/build.properties') }}
          restore-keys: |
            ${{ runner.os }}-sbt-
      
      - name: Build JAR with SBT
        run: |
          sbt clean assembly
      
      - name: Build dependencies JAR
        run: |
          sbt "project deps" assembly
      
      - name: Authenticate to Google Cloud
        uses: google-github-actions/auth@v2
        with:
          credentials_json: ${{ secrets.GCP_SA_KEY }}
      
      - name: Set up Cloud SDK
        uses: google-github-actions/setup-gcloud@v2
      
      - name: Upload JAR to GCS
        run: |
          VERSION=$(git rev-parse --short HEAD)
          TIMESTAMP=$(date +%Y%m%d_%H%M%S)
          JAR_NAME="spark-streaming-local-assembly-${VERSION}-${TIMESTAMP}.jar"
          
          # Upload du JAR principal
          gsutil cp target/scala-2.12/spark-streaming-local-assembly-*.jar \
            gs://${GCS_BUCKET}/jars/${JAR_NAME}
          
          # CrÃ©er un lien symbolique vers latest
          gsutil cp gs://${GCS_BUCKET}/jars/${JAR_NAME} \
            gs://${GCS_BUCKET}/jars/spark-streaming-local-assembly-latest.jar
          
          echo "JAR uploadÃ©: gs://${GCS_BUCKET}/jars/${JAR_NAME}"
          echo "JAR_LATEST=gs://${GCS_BUCKET}/jars/spark-streaming-local-assembly-latest.jar" >> $GITHUB_ENV
      
      - name: Upload dependencies JAR
        run: |
          if [ -f deps/target/scala-2.12/spark-streaming-deps.jar ]; then
            gsutil cp deps/target/scala-2.12/spark-streaming-deps.jar \
              gs://${GCS_BUCKET}/jars/deps/spark-streaming-deps.jar
            echo "Dependencies JAR uploadÃ©"
          fi
      
      - name: Display deployment info
        run: |
          echo "âœ… Build terminÃ© avec succÃ¨s"
          echo "ðŸ“¦ JAR disponible: ${{ env.JAR_LATEST }}"
          echo "ðŸ”§ Pour lancer le job Dataproc, utilisez:"
          echo "   gcloud dataproc jobs submit spark \\"
          echo "     --region=${REGION} \\"
          echo "     --class=com.example.streaming.Consumer \\"
          echo "     --jars=${{ env.JAR_LATEST }} \\"
          echo "     -- <PROJECT_ID> <INPUT_PATH> <BIGQUERY_TABLE> <CHECKPOINT_PATH>"



